{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a89f7c3b-b8a4-4278-b95d-d761fbc43dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/diffusers/src')\n",
    "\n",
    "from diffusers.models.attention_processor import (\n",
    "    Attention,\n",
    "    AttentionProcessor,\n",
    "    FluxAttnProcessor2_0,\n",
    "    FluxAttnProcessor2_0_NPU,\n",
    "    FusedFluxAttnProcessor2_0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074c86b3-915a-4108-9ae7-30e0e4a99a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionXLPipeline\n",
    "\n",
    "sdxl_pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"/mnt/d/studying-code/modelscope/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16\n",
    ")\n",
    "sdxl_pipe = sdxl_pipe.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2587d28e-d87d-4c01-ae70-948bfb468711",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in sdxl_pipe.unet.attn_processors.keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e729830c-2104-48e6-a0d2-211ece4895aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import AutoPipelineForText2Image\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "# 加载基础模型管道\n",
    "flux_pipe = AutoPipelineForText2Image.from_pretrained(\"/mnt/d/studying-code/modelscope/FLUX.1-dev\", torch_dtype=torch.bfloat16)\n",
    "flux_pipe.enable_sequential_cpu_offload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c27b334-d7f6-4fd7-9077-fe05139de514",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_processor_dict = {}\n",
    "for k in flux_pipe.transformer.attn_processors.keys():\n",
    "    if we_want_to_modify(k):\n",
    "        attn_processor_dict[k] = MyAttnProcessor()\n",
    "    else:\n",
    "        attn_processor_dict[k] = AttnProcessor()\n",
    "\n",
    "unet.set_attn_processor(attn_processor_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b891794b-6767-4c08-8ab2-d3d3922a7ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in flux_pipe.transformer.attn_processors.keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "74b03c2a-197c-4293-a7a5-f0b6b24fb78e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AttnProcessor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mEmbeddingInsertAttnProcessor\u001b[39;00m(\u001b[43mAttnProcessor\u001b[49m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AttnProcessor' is not defined"
     ]
    }
   ],
   "source": [
    "class EmbeddingInsertAttnProcessor(AttnProcessor):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.first_maps = {}\n",
    "        self.prev_maps = {}\n",
    "\n",
    "    def __call__(self, attn: Attention, hidden_states, encoder_hidden_states=None, **kwargs):\n",
    "        if encoder_hidden_states is None:\n",
    "            # Is self attention\n",
    "            res = super().__call__(attn, hidden_states, encoder_hidden_states, **kwargs)\n",
    "            cross_map = torch.cat((self.first_maps[t], self.prev_maps[t]), dim=1)\n",
    "            res = super().__call__(attn, hidden_states, cross_map, **kwargs)\n",
    "    \n",
    "        else:\n",
    "            # Is cross attention\n",
    "            cross_map = torch.cat((self.first_maps[t], self.prev_maps[t]), dim=1)\n",
    "            res = super().__call__(attn, hidden_states, cross_map, **kwargs)\n",
    "    \n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7ab5fa-c229-4742-acdc-44fbc6b6ede2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
