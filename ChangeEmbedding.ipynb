{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a89f7c3b-b8a4-4278-b95d-d761fbc43dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/diffusers/src')\n",
    "\n",
    "from diffusers.models.attention_processor import (\n",
    "    Attention,\n",
    "    AttnProcessor,\n",
    "    AttentionProcessor,\n",
    "    FluxAttnProcessor2_0,\n",
    "    FluxAttnProcessor2_0_NPU,\n",
    "    FusedFluxAttnProcessor2_0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e729830c-2104-48e6-a0d2-211ece4895aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import AutoPipelineForText2Image\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "# 加载基础模型管道\n",
    "flux_pipe = AutoPipelineForText2Image.from_pretrained(\"/mnt/d/studying-code/modelscope/FLUX.1-dev\", torch_dtype=torch.bfloat16)\n",
    "flux_pipe.enable_sequential_cpu_offload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924a19e4-60ff-491c-b4ec-4a00ac0ab16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\"A photo of a bunny\"]\n",
    "\n",
    "image = pipe(\n",
    "    prompt,\n",
    "    guidance_scale=0.0,\n",
    "    num_inference_steps=4,\n",
    "    max_sequence_length=256,\n",
    "    height=1024,\n",
    "    width=1024,\n",
    "    generator=torch.Generator(\"cpu\").manual_seed(0)\n",
    ").images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b891794b-6767-4c08-8ab2-d3d3922a7ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in flux_pipe.transformer.attn_processors.keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74b03c2a-197c-4293-a7a5-f0b6b24fb78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingInsertAttnProcessor(AttnProcessor):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.first_maps = {}\n",
    "        self.prev_maps = {}\n",
    "\n",
    "    def __call__(self, attn: Attention, hidden_states, encoder_hidden_states=None, **kwargs):\n",
    "        if encoder_hidden_states is None:\n",
    "            # Is self attention\n",
    "            res = super().__call__(attn, hidden_states, encoder_hidden_states, **kwargs)\n",
    "            cross_map = torch.cat((self.first_maps[t], self.prev_maps[t]), dim=1)\n",
    "            res = super().__call__(attn, hidden_states, cross_map, **kwargs)\n",
    "    \n",
    "        else:\n",
    "            # Is cross attention\n",
    "            cross_map = torch.cat((self.first_maps[t], self.prev_maps[t]), dim=1)\n",
    "            res = super().__call__(attn, hidden_states, cross_map, **kwargs)\n",
    "    \n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb7ab5fa-c229-4742-acdc-44fbc6b6ede2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 FluxTransformerBlock(\n",
      "  (norm1): AdaLayerNormZero(\n",
      "    (silu): SiLU()\n",
      "    (linear): Linear(in_features=3072, out_features=18432, bias=True)\n",
      "    (norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  )\n",
      "  (norm1_context): AdaLayerNormZero(\n",
      "    (silu): SiLU()\n",
      "    (linear): Linear(in_features=3072, out_features=18432, bias=True)\n",
      "    (norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  )\n",
      "  (attn): Attention(\n",
      "    (norm_q): RMSNorm()\n",
      "    (norm_k): RMSNorm()\n",
      "    (to_q): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (to_k): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (to_v): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (add_k_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (add_v_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (add_q_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (to_out): ModuleList(\n",
      "      (0): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (to_add_out): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (norm_added_q): RMSNorm()\n",
      "    (norm_added_k): RMSNorm()\n",
      "  )\n",
      "  (norm2): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  (ff): FeedForward(\n",
      "    (net): ModuleList(\n",
      "      (0): GELU(\n",
      "        (proj): Linear(in_features=3072, out_features=12288, bias=True)\n",
      "      )\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "      (2): Linear(in_features=12288, out_features=3072, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (norm2_context): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  (ff_context): FeedForward(\n",
      "    (net): ModuleList(\n",
      "      (0): GELU(\n",
      "        (proj): Linear(in_features=3072, out_features=12288, bias=True)\n",
      "      )\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "      (2): Linear(in_features=12288, out_features=3072, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "1 FluxTransformerBlock(\n",
      "  (norm1): AdaLayerNormZero(\n",
      "    (silu): SiLU()\n",
      "    (linear): Linear(in_features=3072, out_features=18432, bias=True)\n",
      "    (norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  )\n",
      "  (norm1_context): AdaLayerNormZero(\n",
      "    (silu): SiLU()\n",
      "    (linear): Linear(in_features=3072, out_features=18432, bias=True)\n",
      "    (norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  )\n",
      "  (attn): Attention(\n",
      "    (norm_q): RMSNorm()\n",
      "    (norm_k): RMSNorm()\n",
      "    (to_q): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (to_k): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (to_v): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (add_k_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (add_v_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (add_q_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (to_out): ModuleList(\n",
      "      (0): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (to_add_out): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (norm_added_q): RMSNorm()\n",
      "    (norm_added_k): RMSNorm()\n",
      "  )\n",
      "  (norm2): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  (ff): FeedForward(\n",
      "    (net): ModuleList(\n",
      "      (0): GELU(\n",
      "        (proj): Linear(in_features=3072, out_features=12288, bias=True)\n",
      "      )\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "      (2): Linear(in_features=12288, out_features=3072, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (norm2_context): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  (ff_context): FeedForward(\n",
      "    (net): ModuleList(\n",
      "      (0): GELU(\n",
      "        (proj): Linear(in_features=3072, out_features=12288, bias=True)\n",
      "      )\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "      (2): Linear(in_features=12288, out_features=3072, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "2 FluxTransformerBlock(\n",
      "  (norm1): AdaLayerNormZero(\n",
      "    (silu): SiLU()\n",
      "    (linear): Linear(in_features=3072, out_features=18432, bias=True)\n",
      "    (norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  )\n",
      "  (norm1_context): AdaLayerNormZero(\n",
      "    (silu): SiLU()\n",
      "    (linear): Linear(in_features=3072, out_features=18432, bias=True)\n",
      "    (norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  )\n",
      "  (attn): Attention(\n",
      "    (norm_q): RMSNorm()\n",
      "    (norm_k): RMSNorm()\n",
      "    (to_q): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (to_k): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (to_v): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (add_k_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (add_v_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (add_q_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (to_out): ModuleList(\n",
      "      (0): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (to_add_out): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (norm_added_q): RMSNorm()\n",
      "    (norm_added_k): RMSNorm()\n",
      "  )\n",
      "  (norm2): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  (ff): FeedForward(\n",
      "    (net): ModuleList(\n",
      "      (0): GELU(\n",
      "        (proj): Linear(in_features=3072, out_features=12288, bias=True)\n",
      "      )\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "      (2): Linear(in_features=12288, out_features=3072, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (norm2_context): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  (ff_context): FeedForward(\n",
      "    (net): ModuleList(\n",
      "      (0): GELU(\n",
      "        (proj): Linear(in_features=3072, out_features=12288, bias=True)\n",
      "      )\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "      (2): Linear(in_features=12288, out_features=3072, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "3 FluxTransformerBlock(\n",
      "  (norm1): AdaLayerNormZero(\n",
      "    (silu): SiLU()\n",
      "    (linear): Linear(in_features=3072, out_features=18432, bias=True)\n",
      "    (norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  )\n",
      "  (norm1_context): AdaLayerNormZero(\n",
      "    (silu): SiLU()\n",
      "    (linear): Linear(in_features=3072, out_features=18432, bias=True)\n",
      "    (norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  )\n",
      "  (attn): Attention(\n",
      "    (norm_q): RMSNorm()\n",
      "    (norm_k): RMSNorm()\n",
      "    (to_q): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (to_k): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (to_v): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (add_k_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (add_v_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (add_q_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (to_out): ModuleList(\n",
      "      (0): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (to_add_out): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (norm_added_q): RMSNorm()\n",
      "    (norm_added_k): RMSNorm()\n",
      "  )\n",
      "  (norm2): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  (ff): FeedForward(\n",
      "    (net): ModuleList(\n",
      "      (0): GELU(\n",
      "        (proj): Linear(in_features=3072, out_features=12288, bias=True)\n",
      "      )\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "      (2): Linear(in_features=12288, out_features=3072, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (norm2_context): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  (ff_context): FeedForward(\n",
      "    (net): ModuleList(\n",
      "      (0): GELU(\n",
      "        (proj): Linear(in_features=3072, out_features=12288, bias=True)\n",
      "      )\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "      (2): Linear(in_features=12288, out_features=3072, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "4 FluxTransformerBlock(\n",
      "  (norm1): AdaLayerNormZero(\n",
      "    (silu): SiLU()\n",
      "    (linear): Linear(in_features=3072, out_features=18432, bias=True)\n",
      "    (norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  )\n",
      "  (norm1_context): AdaLayerNormZero(\n",
      "    (silu): SiLU()\n",
      "    (linear): Linear(in_features=3072, out_features=18432, bias=True)\n",
      "    (norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  )\n",
      "  (attn): Attention(\n",
      "    (norm_q): RMSNorm()\n",
      "    (norm_k): RMSNorm()\n",
      "    (to_q): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (to_k): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (to_v): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (add_k_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (add_v_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (add_q_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (to_out): ModuleList(\n",
      "      (0): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (to_add_out): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (norm_added_q): RMSNorm()\n",
      "    (norm_added_k): RMSNorm()\n",
      "  )\n",
      "  (norm2): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  (ff): FeedForward(\n",
      "    (net): ModuleList(\n",
      "      (0): GELU(\n",
      "        (proj): Linear(in_features=3072, out_features=12288, bias=True)\n",
      "      )\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "      (2): Linear(in_features=12288, out_features=3072, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (norm2_context): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  (ff_context): FeedForward(\n",
      "    (net): ModuleList(\n",
      "      (0): GELU(\n",
      "        (proj): Linear(in_features=3072, out_features=12288, bias=True)\n",
      "      )\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "      (2): Linear(in_features=12288, out_features=3072, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "5 FluxTransformerBlock(\n",
      "  (norm1): AdaLayerNormZero(\n",
      "    (silu): SiLU()\n",
      "    (linear): Linear(in_features=3072, out_features=18432, bias=True)\n",
      "    (norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  )\n",
      "  (norm1_context): AdaLayerNormZero(\n",
      "    (silu): SiLU()\n",
      "    (linear): Linear(in_features=3072, out_features=18432, bias=True)\n",
      "    (norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  )\n",
      "  (attn): Attention(\n",
      "    (norm_q): RMSNorm()\n",
      "    (norm_k): RMSNorm()\n",
      "    (to_q): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (to_k): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (to_v): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (add_k_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (add_v_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (add_q_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (to_out): ModuleList(\n",
      "      (0): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (to_add_out): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (norm_added_q): RMSNorm()\n",
      "    (norm_added_k): RMSNorm()\n",
      "  )\n",
      "  (norm2): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  (ff): FeedForward(\n",
      "    (net): ModuleList(\n",
      "      (0): GELU(\n",
      "        (proj): Linear(in_features=3072, out_features=12288, bias=True)\n",
      "      )\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "      (2): Linear(in_features=12288, out_features=3072, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (norm2_context): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  (ff_context): FeedForward(\n",
      "    (net): ModuleList(\n",
      "      (0): GELU(\n",
      "        (proj): Linear(in_features=3072, out_features=12288, bias=True)\n",
      "      )\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "      (2): Linear(in_features=12288, out_features=3072, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "6 FluxTransformerBlock(\n",
      "  (norm1): AdaLayerNormZero(\n",
      "    (silu): SiLU()\n",
      "    (linear): Linear(in_features=3072, out_features=18432, bias=True)\n",
      "    (norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  )\n",
      "  (norm1_context): AdaLayerNormZero(\n",
      "    (silu): SiLU()\n",
      "    (linear): Linear(in_features=3072, out_features=18432, bias=True)\n",
      "    (norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  )\n",
      "  (attn): Attention(\n",
      "    (norm_q): RMSNorm()\n",
      "    (norm_k): RMSNorm()\n",
      "    (to_q): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (to_k): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (to_v): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (add_k_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (add_v_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (add_q_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (to_out): ModuleList(\n",
      "      (0): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (to_add_out): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (norm_added_q): RMSNorm()\n",
      "    (norm_added_k): RMSNorm()\n",
      "  )\n",
      "  (norm2): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  (ff): FeedForward(\n",
      "    (net): ModuleList(\n",
      "      (0): GELU(\n",
      "        (proj): Linear(in_features=3072, out_features=12288, bias=True)\n",
      "      )\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "      (2): Linear(in_features=12288, out_features=3072, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (norm2_context): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  (ff_context): FeedForward(\n",
      "    (net): ModuleList(\n",
      "      (0): GELU(\n",
      "        (proj): Linear(in_features=3072, out_features=12288, bias=True)\n",
      "      )\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "      (2): Linear(in_features=12288, out_features=3072, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "7 FluxTransformerBlock(\n",
      "  (norm1): AdaLayerNormZero(\n",
      "    (silu): SiLU()\n",
      "    (linear): Linear(in_features=3072, out_features=18432, bias=True)\n",
      "    (norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  )\n",
      "  (norm1_context): AdaLayerNormZero(\n",
      "    (silu): SiLU()\n",
      "    (linear): Linear(in_features=3072, out_features=18432, bias=True)\n",
      "    (norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  )\n",
      "  (attn): Attention(\n",
      "    (norm_q): RMSNorm()\n",
      "    (norm_k): RMSNorm()\n",
      "    (to_q): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (to_k): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (to_v): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (add_k_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (add_v_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (add_q_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (to_out): ModuleList(\n",
      "      (0): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (to_add_out): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (norm_added_q): RMSNorm()\n",
      "    (norm_added_k): RMSNorm()\n",
      "  )\n",
      "  (norm2): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  (ff): FeedForward(\n",
      "    (net): ModuleList(\n",
      "      (0): GELU(\n",
      "        (proj): Linear(in_features=3072, out_features=12288, bias=True)\n",
      "      )\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "      (2): Linear(in_features=12288, out_features=3072, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (norm2_context): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  (ff_context): FeedForward(\n",
      "    (net): ModuleList(\n",
      "      (0): GELU(\n",
      "        (proj): Linear(in_features=3072, out_features=12288, bias=True)\n",
      "      )\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "      (2): Linear(in_features=12288, out_features=3072, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "8 FluxTransformerBlock(\n",
      "  (norm1): AdaLayerNormZero(\n",
      "    (silu): SiLU()\n",
      "    (linear): Linear(in_features=3072, out_features=18432, bias=True)\n",
      "    (norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  )\n",
      "  (norm1_context): AdaLayerNormZero(\n",
      "    (silu): SiLU()\n",
      "    (linear): Linear(in_features=3072, out_features=18432, bias=True)\n",
      "    (norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  )\n",
      "  (attn): Attention(\n",
      "    (norm_q): RMSNorm()\n",
      "    (norm_k): RMSNorm()\n",
      "    (to_q): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (to_k): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (to_v): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (add_k_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (add_v_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (add_q_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (to_out): ModuleList(\n",
      "      (0): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (to_add_out): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (norm_added_q): RMSNorm()\n",
      "    (norm_added_k): RMSNorm()\n",
      "  )\n",
      "  (norm2): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  (ff): FeedForward(\n",
      "    (net): ModuleList(\n",
      "      (0): GELU(\n",
      "        (proj): Linear(in_features=3072, out_features=12288, bias=True)\n",
      "      )\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "      (2): Linear(in_features=12288, out_features=3072, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (norm2_context): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  (ff_context): FeedForward(\n",
      "    (net): ModuleList(\n",
      "      (0): GELU(\n",
      "        (proj): Linear(in_features=3072, out_features=12288, bias=True)\n",
      "      )\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "      (2): Linear(in_features=12288, out_features=3072, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "9 FluxTransformerBlock(\n",
      "  (norm1): AdaLayerNormZero(\n",
      "    (silu): SiLU()\n",
      "    (linear): Linear(in_features=3072, out_features=18432, bias=True)\n",
      "    (norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  )\n",
      "  (norm1_context): AdaLayerNormZero(\n",
      "    (silu): SiLU()\n",
      "    (linear): Linear(in_features=3072, out_features=18432, bias=True)\n",
      "    (norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  )\n",
      "  (attn): Attention(\n",
      "    (norm_q): RMSNorm()\n",
      "    (norm_k): RMSNorm()\n",
      "    (to_q): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (to_k): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (to_v): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (add_k_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (add_v_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (add_q_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (to_out): ModuleList(\n",
      "      (0): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (to_add_out): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (norm_added_q): RMSNorm()\n",
      "    (norm_added_k): RMSNorm()\n",
      "  )\n",
      "  (norm2): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  (ff): FeedForward(\n",
      "    (net): ModuleList(\n",
      "      (0): GELU(\n",
      "        (proj): Linear(in_features=3072, out_features=12288, bias=True)\n",
      "      )\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "      (2): Linear(in_features=12288, out_features=3072, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (norm2_context): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  (ff_context): FeedForward(\n",
      "    (net): ModuleList(\n",
      "      (0): GELU(\n",
      "        (proj): Linear(in_features=3072, out_features=12288, bias=True)\n",
      "      )\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "      (2): Linear(in_features=12288, out_features=3072, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "10 FluxTransformerBlock(\n",
      "  (norm1): AdaLayerNormZero(\n",
      "    (silu): SiLU()\n",
      "    (linear): Linear(in_features=3072, out_features=18432, bias=True)\n",
      "    (norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  )\n",
      "  (norm1_context): AdaLayerNormZero(\n",
      "    (silu): SiLU()\n",
      "    (linear): Linear(in_features=3072, out_features=18432, bias=True)\n",
      "    (norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  )\n",
      "  (attn): Attention(\n",
      "    (norm_q): RMSNorm()\n",
      "    (norm_k): RMSNorm()\n",
      "    (to_q): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (to_k): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (to_v): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (add_k_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (add_v_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (add_q_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (to_out): ModuleList(\n",
      "      (0): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (to_add_out): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (norm_added_q): RMSNorm()\n",
      "    (norm_added_k): RMSNorm()\n",
      "  )\n",
      "  (norm2): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  (ff): FeedForward(\n",
      "    (net): ModuleList(\n",
      "      (0): GELU(\n",
      "        (proj): Linear(in_features=3072, out_features=12288, bias=True)\n",
      "      )\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "      (2): Linear(in_features=12288, out_features=3072, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (norm2_context): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  (ff_context): FeedForward(\n",
      "    (net): ModuleList(\n",
      "      (0): GELU(\n",
      "        (proj): Linear(in_features=3072, out_features=12288, bias=True)\n",
      "      )\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "      (2): Linear(in_features=12288, out_features=3072, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "11 FluxTransformerBlock(\n",
      "  (norm1): AdaLayerNormZero(\n",
      "    (silu): SiLU()\n",
      "    (linear): Linear(in_features=3072, out_features=18432, bias=True)\n",
      "    (norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  )\n",
      "  (norm1_context): AdaLayerNormZero(\n",
      "    (silu): SiLU()\n",
      "    (linear): Linear(in_features=3072, out_features=18432, bias=True)\n",
      "    (norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  )\n",
      "  (attn): Attention(\n",
      "    (norm_q): RMSNorm()\n",
      "    (norm_k): RMSNorm()\n",
      "    (to_q): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (to_k): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (to_v): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (add_k_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (add_v_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (add_q_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (to_out): ModuleList(\n",
      "      (0): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (to_add_out): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (norm_added_q): RMSNorm()\n",
      "    (norm_added_k): RMSNorm()\n",
      "  )\n",
      "  (norm2): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  (ff): FeedForward(\n",
      "    (net): ModuleList(\n",
      "      (0): GELU(\n",
      "        (proj): Linear(in_features=3072, out_features=12288, bias=True)\n",
      "      )\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "      (2): Linear(in_features=12288, out_features=3072, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (norm2_context): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  (ff_context): FeedForward(\n",
      "    (net): ModuleList(\n",
      "      (0): GELU(\n",
      "        (proj): Linear(in_features=3072, out_features=12288, bias=True)\n",
      "      )\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "      (2): Linear(in_features=12288, out_features=3072, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "12 FluxTransformerBlock(\n",
      "  (norm1): AdaLayerNormZero(\n",
      "    (silu): SiLU()\n",
      "    (linear): Linear(in_features=3072, out_features=18432, bias=True)\n",
      "    (norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  )\n",
      "  (norm1_context): AdaLayerNormZero(\n",
      "    (silu): SiLU()\n",
      "    (linear): Linear(in_features=3072, out_features=18432, bias=True)\n",
      "    (norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  )\n",
      "  (attn): Attention(\n",
      "    (norm_q): RMSNorm()\n",
      "    (norm_k): RMSNorm()\n",
      "    (to_q): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (to_k): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (to_v): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (add_k_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (add_v_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (add_q_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (to_out): ModuleList(\n",
      "      (0): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (to_add_out): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (norm_added_q): RMSNorm()\n",
      "    (norm_added_k): RMSNorm()\n",
      "  )\n",
      "  (norm2): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  (ff): FeedForward(\n",
      "    (net): ModuleList(\n",
      "      (0): GELU(\n",
      "        (proj): Linear(in_features=3072, out_features=12288, bias=True)\n",
      "      )\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "      (2): Linear(in_features=12288, out_features=3072, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (norm2_context): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  (ff_context): FeedForward(\n",
      "    (net): ModuleList(\n",
      "      (0): GELU(\n",
      "        (proj): Linear(in_features=3072, out_features=12288, bias=True)\n",
      "      )\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "      (2): Linear(in_features=12288, out_features=3072, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "13 FluxTransformerBlock(\n",
      "  (norm1): AdaLayerNormZero(\n",
      "    (silu): SiLU()\n",
      "    (linear): Linear(in_features=3072, out_features=18432, bias=True)\n",
      "    (norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  )\n",
      "  (norm1_context): AdaLayerNormZero(\n",
      "    (silu): SiLU()\n",
      "    (linear): Linear(in_features=3072, out_features=18432, bias=True)\n",
      "    (norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  )\n",
      "  (attn): Attention(\n",
      "    (norm_q): RMSNorm()\n",
      "    (norm_k): RMSNorm()\n",
      "    (to_q): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (to_k): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (to_v): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (add_k_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (add_v_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (add_q_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (to_out): ModuleList(\n",
      "      (0): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (to_add_out): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (norm_added_q): RMSNorm()\n",
      "    (norm_added_k): RMSNorm()\n",
      "  )\n",
      "  (norm2): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  (ff): FeedForward(\n",
      "    (net): ModuleList(\n",
      "      (0): GELU(\n",
      "        (proj): Linear(in_features=3072, out_features=12288, bias=True)\n",
      "      )\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "      (2): Linear(in_features=12288, out_features=3072, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (norm2_context): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  (ff_context): FeedForward(\n",
      "    (net): ModuleList(\n",
      "      (0): GELU(\n",
      "        (proj): Linear(in_features=3072, out_features=12288, bias=True)\n",
      "      )\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "      (2): Linear(in_features=12288, out_features=3072, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "14 FluxTransformerBlock(\n",
      "  (norm1): AdaLayerNormZero(\n",
      "    (silu): SiLU()\n",
      "    (linear): Linear(in_features=3072, out_features=18432, bias=True)\n",
      "    (norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  )\n",
      "  (norm1_context): AdaLayerNormZero(\n",
      "    (silu): SiLU()\n",
      "    (linear): Linear(in_features=3072, out_features=18432, bias=True)\n",
      "    (norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  )\n",
      "  (attn): Attention(\n",
      "    (norm_q): RMSNorm()\n",
      "    (norm_k): RMSNorm()\n",
      "    (to_q): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (to_k): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (to_v): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (add_k_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (add_v_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (add_q_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (to_out): ModuleList(\n",
      "      (0): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (to_add_out): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (norm_added_q): RMSNorm()\n",
      "    (norm_added_k): RMSNorm()\n",
      "  )\n",
      "  (norm2): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  (ff): FeedForward(\n",
      "    (net): ModuleList(\n",
      "      (0): GELU(\n",
      "        (proj): Linear(in_features=3072, out_features=12288, bias=True)\n",
      "      )\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "      (2): Linear(in_features=12288, out_features=3072, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (norm2_context): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  (ff_context): FeedForward(\n",
      "    (net): ModuleList(\n",
      "      (0): GELU(\n",
      "        (proj): Linear(in_features=3072, out_features=12288, bias=True)\n",
      "      )\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "      (2): Linear(in_features=12288, out_features=3072, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "15 FluxTransformerBlock(\n",
      "  (norm1): AdaLayerNormZero(\n",
      "    (silu): SiLU()\n",
      "    (linear): Linear(in_features=3072, out_features=18432, bias=True)\n",
      "    (norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  )\n",
      "  (norm1_context): AdaLayerNormZero(\n",
      "    (silu): SiLU()\n",
      "    (linear): Linear(in_features=3072, out_features=18432, bias=True)\n",
      "    (norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  )\n",
      "  (attn): Attention(\n",
      "    (norm_q): RMSNorm()\n",
      "    (norm_k): RMSNorm()\n",
      "    (to_q): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (to_k): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (to_v): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (add_k_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (add_v_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (add_q_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (to_out): ModuleList(\n",
      "      (0): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (to_add_out): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (norm_added_q): RMSNorm()\n",
      "    (norm_added_k): RMSNorm()\n",
      "  )\n",
      "  (norm2): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  (ff): FeedForward(\n",
      "    (net): ModuleList(\n",
      "      (0): GELU(\n",
      "        (proj): Linear(in_features=3072, out_features=12288, bias=True)\n",
      "      )\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "      (2): Linear(in_features=12288, out_features=3072, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (norm2_context): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  (ff_context): FeedForward(\n",
      "    (net): ModuleList(\n",
      "      (0): GELU(\n",
      "        (proj): Linear(in_features=3072, out_features=12288, bias=True)\n",
      "      )\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "      (2): Linear(in_features=12288, out_features=3072, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "16 FluxTransformerBlock(\n",
      "  (norm1): AdaLayerNormZero(\n",
      "    (silu): SiLU()\n",
      "    (linear): Linear(in_features=3072, out_features=18432, bias=True)\n",
      "    (norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  )\n",
      "  (norm1_context): AdaLayerNormZero(\n",
      "    (silu): SiLU()\n",
      "    (linear): Linear(in_features=3072, out_features=18432, bias=True)\n",
      "    (norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  )\n",
      "  (attn): Attention(\n",
      "    (norm_q): RMSNorm()\n",
      "    (norm_k): RMSNorm()\n",
      "    (to_q): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (to_k): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (to_v): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (add_k_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (add_v_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (add_q_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (to_out): ModuleList(\n",
      "      (0): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (to_add_out): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (norm_added_q): RMSNorm()\n",
      "    (norm_added_k): RMSNorm()\n",
      "  )\n",
      "  (norm2): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  (ff): FeedForward(\n",
      "    (net): ModuleList(\n",
      "      (0): GELU(\n",
      "        (proj): Linear(in_features=3072, out_features=12288, bias=True)\n",
      "      )\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "      (2): Linear(in_features=12288, out_features=3072, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (norm2_context): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  (ff_context): FeedForward(\n",
      "    (net): ModuleList(\n",
      "      (0): GELU(\n",
      "        (proj): Linear(in_features=3072, out_features=12288, bias=True)\n",
      "      )\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "      (2): Linear(in_features=12288, out_features=3072, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "17 FluxTransformerBlock(\n",
      "  (norm1): AdaLayerNormZero(\n",
      "    (silu): SiLU()\n",
      "    (linear): Linear(in_features=3072, out_features=18432, bias=True)\n",
      "    (norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  )\n",
      "  (norm1_context): AdaLayerNormZero(\n",
      "    (silu): SiLU()\n",
      "    (linear): Linear(in_features=3072, out_features=18432, bias=True)\n",
      "    (norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  )\n",
      "  (attn): Attention(\n",
      "    (norm_q): RMSNorm()\n",
      "    (norm_k): RMSNorm()\n",
      "    (to_q): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (to_k): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (to_v): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (add_k_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (add_v_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (add_q_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (to_out): ModuleList(\n",
      "      (0): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (to_add_out): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (norm_added_q): RMSNorm()\n",
      "    (norm_added_k): RMSNorm()\n",
      "  )\n",
      "  (norm2): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  (ff): FeedForward(\n",
      "    (net): ModuleList(\n",
      "      (0): GELU(\n",
      "        (proj): Linear(in_features=3072, out_features=12288, bias=True)\n",
      "      )\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "      (2): Linear(in_features=12288, out_features=3072, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (norm2_context): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  (ff_context): FeedForward(\n",
      "    (net): ModuleList(\n",
      "      (0): GELU(\n",
      "        (proj): Linear(in_features=3072, out_features=12288, bias=True)\n",
      "      )\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "      (2): Linear(in_features=12288, out_features=3072, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "18 FluxTransformerBlock(\n",
      "  (norm1): AdaLayerNormZero(\n",
      "    (silu): SiLU()\n",
      "    (linear): Linear(in_features=3072, out_features=18432, bias=True)\n",
      "    (norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  )\n",
      "  (norm1_context): AdaLayerNormZero(\n",
      "    (silu): SiLU()\n",
      "    (linear): Linear(in_features=3072, out_features=18432, bias=True)\n",
      "    (norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  )\n",
      "  (attn): Attention(\n",
      "    (norm_q): RMSNorm()\n",
      "    (norm_k): RMSNorm()\n",
      "    (to_q): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (to_k): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (to_v): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (add_k_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (add_v_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (add_q_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (to_out): ModuleList(\n",
      "      (0): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (to_add_out): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "    (norm_added_q): RMSNorm()\n",
      "    (norm_added_k): RMSNorm()\n",
      "  )\n",
      "  (norm2): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  (ff): FeedForward(\n",
      "    (net): ModuleList(\n",
      "      (0): GELU(\n",
      "        (proj): Linear(in_features=3072, out_features=12288, bias=True)\n",
      "      )\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "      (2): Linear(in_features=12288, out_features=3072, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (norm2_context): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
      "  (ff_context): FeedForward(\n",
      "    (net): ModuleList(\n",
      "      (0): GELU(\n",
      "        (proj): Linear(in_features=3072, out_features=12288, bias=True)\n",
      "      )\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "      (2): Linear(in_features=12288, out_features=3072, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for index_block, block in enumerate(flux_pipe.transformer.transformer_blocks):\n",
    "    encoder_hidden_states, hidden_states = block(\n",
    "        hidden_states=hidden_states,\n",
    "        encoder_hidden_states=encoder_hidden_states,\n",
    "        temb=temb,\n",
    "        image_rotary_emb=image_rotary_emb,\n",
    "        joint_attention_kwargs=joint_attention_kwargs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd185f7-3754-4460-8b4f-604568947eab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
